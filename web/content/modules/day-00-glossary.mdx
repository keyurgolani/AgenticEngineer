---
title: "AI & Agents Glossary"
description: "A comprehensive reference of key terms, concepts, and acronyms used throughout this course."
week: 1
day: 0
---

<ReadingTime minutes={15} />

# AI & Agents Glossary

This glossary provides definitions for all key terms used in this course. Bookmark this page for quick reference as you progress through the modules.

<Callout type="tip">
  **Pro Tip:** Use your browser's find function (Ctrl/Cmd + F) to quickly search
  for specific terms.
</Callout>

---

## A

### Agent

A software system that uses an LLM as its reasoning engine while providing additional capabilities like tool use, memory, and control flow. Unlike a chatbot that only generates text, an agent can take actions in the world.

### Agentic Loop

The cyclical process agents use: Perceive → Reason → Act → Observe → Repeat. This allows agents to handle uncertainty and retry failed operations.

### API (Application Programming Interface)

A set of rules that allows different software applications to communicate. In AI, APIs let you send prompts to models and receive responses.

### Attention Mechanism

The core innovation in Transformers that allows the model to focus on relevant parts of the input when generating each output token. It's how LLMs understand context and relationships between words.

---

## B

### Backpropagation

The algorithm used to train neural networks by calculating how much each parameter contributed to the error and adjusting accordingly.

### BM25

A keyword-based ranking algorithm used in traditional search engines. Often combined with vector search in hybrid RAG systems.

---

## C

### Chain

A sequence of LLM calls or operations executed in order. Simpler than agents but can't handle loops or retries.

### Checkpointing

Saving the state of an agent at specific points so it can be resumed later or rolled back if something goes wrong.

### Chunking

The process of splitting documents into smaller pieces for indexing in a RAG system. Good chunking preserves semantic meaning.

### Claude

Anthropic's family of LLMs, known for strong reasoning and safety features. Claude 3.5 Sonnet is commonly used for coding tasks.

### Context Engineering

The strategic process of selecting, formatting, and managing information provided to an LLM. Includes deciding what to include, exclude, and how to structure prompts.

### Context Poisoning

When irrelevant or conflicting information in the context confuses the model, degrading its performance.

### Context Rot

The gradual degradation of model performance as the context window fills with irrelevant information over long conversations.

### Context Window

The maximum amount of text (measured in tokens) that an LLM can process at once. Includes both input and output.

### CrewAI

A framework for building multi-agent systems using role-based collaboration. Known for intuitive design and fast execution.

---

## D

### DAG (Directed Acyclic Graph)

A one-way pipeline with no loops. Traditional software architecture that doesn't work well for agents because it can't handle retries.

### Deep Learning

A subset of machine learning using neural networks with multiple layers to learn complex patterns from data.

### Deterministic

Always producing the same output for the same input. Traditional software is deterministic; LLMs are probabilistic.

---

## E

### E2B (Code Interpreter)

A sandboxed environment for safely executing code generated by LLMs. Prevents malicious code from affecting your system.

### Embedding

A numerical representation (vector) of text that captures semantic meaning. Similar concepts have similar embeddings.

### Embedding Model

A model that converts text into embedding vectors. Examples: text-embedding-3-small (OpenAI), all-MiniLM-L6-v2 (open source).

### Episodic Memory

An agent's memory of past experiences and interactions, typically stored in a vector database for retrieval.

---

## F

### Few-Shot Learning

Providing a few examples in the prompt to help the model understand the desired output format or behavior.

### Fine-Tuning

Training a pre-trained model further on specific data to specialize it for a particular task or domain.

---

## G

### GPT (Generative Pre-trained Transformer)

OpenAI's family of LLMs. GPT-4 and GPT-4o are commonly used for agent development.

### Graph (in LangGraph)

A data structure where nodes represent operations and edges represent control flow. LangGraph uses graphs to model agent behavior.

---

## H

### Hallucination

When an LLM generates content that is factually incorrect or not grounded in the provided context. A fundamental limitation of current models.

### HITL (Human-in-the-Loop)

A pattern where agents pause for human approval before taking critical actions. Essential for production safety.

### Hybrid Search

Combining keyword-based search (BM25) with semantic search (vectors) for better retrieval results.

---

## I

### Inference

The process of generating outputs from a trained model, as opposed to training the model.

### Isolation (Context)

Giving each agent or sub-agent only the minimum information it needs, preventing context poisoning.

---

## L

### LangChain

A popular framework for building LLM applications, providing abstractions for prompts, chains, and tool use.

### LangGraph

A library built on LangChain for creating stateful, graph-based agent workflows. The primary framework used in this course.

### LangSmith

A platform for debugging, testing, and monitoring LLM applications. Provides detailed execution traces.

### LLM (Large Language Model)

A neural network trained on massive text data to understand and generate human language. Examples: GPT-4, Claude, Llama.

---

## M

### MCP (Model Context Protocol)

An open standard for connecting AI systems to external tools and data sources. Think of it as "USB for AI."

### Memory (Agent)

Systems that allow agents to persist information across interactions. Includes working memory, episodic memory, and semantic memory.

### Multi-Agent System

A system where multiple specialized agents collaborate to complete complex tasks, often coordinated by a supervisor agent.

---

## N

### Neural Network

A computing system inspired by biological brains, consisting of layers of interconnected nodes that learn patterns from data.

### Node (in LangGraph)

A Python function that performs work in a graph. Nodes receive state, do something, and return state updates.

---

## O

### OODA Loop

Observe, Orient, Decide, Act—a decision-making framework that maps to the agent cognitive loop.

### Orchestrator

The component that controls agent execution, managing the loop between reasoning and action.

---

## P

### Parameter

A value in a neural network that is learned during training. LLMs have billions of parameters.

### Pipeline

A linear sequence of operations. See DAG.

### Probabilistic

Involving chance or variation. LLMs are probabilistic—the same input might produce different outputs.

### Prompt

The text input sent to an LLM, including instructions, context, and the user's question.

### Prompt Engineering

The practice of crafting effective prompts to get desired outputs from LLMs.

---

## R

### RAG (Retrieval-Augmented Generation)

A technique that retrieves relevant documents from a knowledge base before generating a response, grounding the output in actual data.

### ReAct (Reason + Act)

A prompting pattern where the model alternates between reasoning about what to do and taking actions.

### Recursive Summarization

Compressing older conversation messages into summaries to save context window space while preserving key information.

---

## S

### Sandbox

An isolated environment for executing untrusted code safely. E2B provides sandboxes for agent code execution.

### Semantic Memory

An agent's storage of facts and knowledge, typically in a vector database for semantic retrieval.

### Semantic Search

Finding documents by meaning rather than exact keyword matching, using embedding similarity.

### State (in LangGraph)

A shared dictionary that persists across the agent loop, holding conversation history, intermediate results, and task context.

### Structured Output

Forcing an LLM to output in a specific format (JSON, XML) rather than free-form text.

### Supervisor Pattern

A multi-agent architecture where a supervisor agent coordinates specialized worker agents.

### System Prompt

Hidden instructions that define how an AI should behave, typically set by the developer rather than the user.

---

## T

### Tavily

A search API designed for AI agents, providing clean, structured search results.

### Token

A chunk of text (roughly 4 characters or 3/4 of a word) that LLMs process. Pricing and context limits are measured in tokens.

### Tokenization

The process of converting text into tokens that the model can process.

### Tool

A function that an agent can call to interact with the world—search, code execution, API calls, etc.

### Transformer

The neural network architecture underlying modern LLMs, introduced in the "Attention Is All You Need" paper (2017).

---

## V

### Vector

A list of numbers representing a point in multi-dimensional space. Embeddings are vectors.

### Vector Database

A database optimized for storing and searching embedding vectors. Examples: ChromaDB, LanceDB, Pinecone.

---

## W

### Working Memory

An agent's immediate task context—the current conversation and active goals. Fast but limited (context window).

---

## Z

### Zero-Shot

Asking a model to perform a task without providing examples, relying only on instructions.

---

<Callout type="info">
  **Missing a term?** This glossary covers the core concepts. For
  framework-specific terms, refer to the official documentation for LangGraph,
  LangChain, or MCP.
</Callout>
